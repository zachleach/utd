# Operating Systems: Exam 1

### 2024-02-08: start threads (ch 4)
Introduction to threads, and how they are different from threads. The key distinction is data sharing -- processes have distinct copies of data, whereas threads share a processes data. As a result, a lot of challenges arise.

### 2024-02-13 threads and concurrency
amdahls law is on the test
kernel threads, operating system threads, kthreadd
user threads, "invisible to the os"
servicing user threads (thread models)
many-to-one (uncommon), blocking issue
thread API handles the scheduling of user thread to kthread (not the os)
one-to-one (most common), resource intensive 
many-to-many (common), kthread multiplexing, two-level model
thread libraries (both user and kernel threads)
pthread interface (not implementation) pthread_attr_init(), pthread_create(), pthread_join(), pthread_exit()
java threads, extending thread, runnables, executor interface (java.util.concurrent
java thread pools, newSingleThreadExecutor(), newFixedThreatPool(int size), newCachedThreadPool()
fork join

### 2024-02-15
java thread creation 
async thread cancellation
deferred cancellation
signals
async signal sending
sync signal sending
thread cancellation modes
deferred kernel signal handler
user defined signal handlers
light weight processes (4.6.5)

Asynchronous thread cancellation deletes instantly, deferred cancellation sets a flag to indicate a thread *should* be terminated (allowing for safe termination). Threads being cancelled are called target threads. Asynchronous cancellation is useful primarily for terminating viruses, otherwise deferred should be used.

Signals are always sent to the process, not the thread. Only the parent process can send signals to its spawned threads. 

Processes have 4 options for signal handling: terminate the thread, terminate the entire process, terminate multiple threads.

Threads can generate signals, if the signal is sent to the parent process it's called "synchronous" signal sending; if it's sent to a different process, it's "asynchronous".

You can define signal handlers that can override the kernel's default signal handler.

Professor told story of how he was in charge of the hardware that would be utilized by virtual cloud type systems.


# 2024-02-20 synchronization, critical section problem (software solution)
process suspension bank account example, one value in the computation gets modified during process suspension: __the critical section problem__

when you want to access resource, raise flag. if flag is raised at resource, don't access the resource. __what if both raise the flag at the same time?__ 

__two process lock__ 
suppose two processes want a resource (on a single core, two things can't happen at the same time)
keep two flag bits
when p1 wants to access, it sets flag_1
when p2 wants to access, it sets flag_2
if both flags are set, p1 sets a third variable to p2
wait until flag_2 is false
assume after p2 done accessing, sets flag_2 to false

__3 properties to solving the critical section problem__
1. two processes can't access at the same time (mutually exclusive)
2. make progress (e.g., don't create a deadlock (turn variable))
3. bounded waiting

the core ideas are that processes can be suspended mid operation, and two processes may want to access the same resources. 







# 2024-02-22 
one-flag "atomic" (hardware) solution to the critical section problem, in contrast to last lecture's two-flag (software) solution to the critical section. additionally, one-flag software solution using a variable (the flag) called a "semaphore".

### what are the 3 criteria for a solution to the critical section problem [6.2]
### what does the `test_and_set()` function do and return [6.4.2]
### what does "atomic" mean in the context of synchronization [6.4.2]
### what do 'true' and 'false' values mean for the "target" hardware flag [6.4.2]
### what is a "semaphore" [6.6]
### what does semaphore `wait()` do [6.6]
### what does semaphore `signal()` do [6.6]

hardware implements a boolean flag called "target" at each virtual memory cell. if the flag is true, it's being accessed; if false, it's free to access. 

this works in conjunction with an atomic hardware function called `test_and_set()` that returns whether the memory is occupied (whether the target flag is true or false), and sets target to true. it does both, getting the return value and setting to target to true, at the same time.

mutex `wait()` is a while loop that waits for the semaphore to be set to be 1, mutex `signal()` simply sets the semaphore variable to 1 (notifies the other waiting processes that it's free). 

```TEST_AND_SET() CRITICAL SECTION
do {
	while(test_and_set(target));	/* repeatedly try to access critical section */

	/* access critical section */

	target = false
	break;
} while (true);
```

```SEMAPHORE CRITICAL SECTION
do {
	wait(s);	/* repeatedly try to access critical section */

	/* access critical section */

	signal(s);
	break;
} while (true);
```

end of class question: if wait() is atomic (can't be interrupted), does a wait() block the processor? (e.g., since allegedly, its implemented as a while loop under the hood).

---

# 2024-02-27: synchronization continued

-	mutex lock
-	semaphore wait()
-	semaphore signal()
-	semaphore busy waiting
-	deadlock
-	starvation
-	deadlock 
-	how to solve deadlock
-	counting semaphore
-	bounded buffer problem (empty, mutex, full, producer, consumer)

ravi prakash is the new professor. 

semaphores aren't actually just variables, they are implemented with some extra shit to avoid busy-waiting (spinning in a while loop doing nothing).

a deadlock is when one process waits indefinitely for a signal which can't come from another process (e.g., the other process is waiting for a signal from the waiting process).

when piping, data is written to the "pipe" and then data gets "read" from the pipe. that pipe is a buffer.

there is a thing called a counting semaphore, let a set number of threads access the data sequentially. in fact, this is actually the semaphore implementation we've been using. 

there is a thing called the bounded-buffer problem which can be solved with counting semaphores. the problem is that we shouldn't add to a full buffer. use counting semaphores to keep track of the elements entering and leaving the buffer. use semaphores because they let you wait (for such a condition, a non-full buffer, to be true).


---

# 2024-02-29: start ch 7 - syncrhonization examples

-	7.1.1	bounded buffer problem 
-	7.1.2	readers-writers problem 
-	read-write mutex

the bounded buffer problem has an issue of *how to prevent the semaphore from beind decremented and incremented at the same time*

the readers-writers problem is that multiple users can read at the same, but you can't have write-write or write-read.


>	by this point in the lecture (15 minutes in), i have everything i need to know. the starting point is all you need; because the next lecture starting point will tell you the last lecture's end point (assuming the lecture is following a textbook strictly in order).
>	i am going to leave lecture now to go study on my own.

